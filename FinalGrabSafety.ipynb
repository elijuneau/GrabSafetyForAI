{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OVERVIEW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are 3 Main Process that will be applied in this case :\n",
    "\n",
    "1. Data Preparation and Preprocessing\n",
    "2. Modelling Comparison (7 different types of models, see which is the most sensitive and accurate)\n",
    "3. Hyperparameter Tuning\n",
    "4. Metrics Evaluation for the fixed Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, I downloaded 11 seperate datasets with around a million records each. The datasets can be found at \n",
    "https://github.com/yingjia-liu/Msft-Grab-FRS/blob/master/Safety/Safety_Problem_Statement.md. \n",
    "\n",
    "A https://msftgrab.z23.web.core.windows.net/safety/Safety_DataSet_Aggregated.zip (full aggregated dataframe) was available, but is corrupted upon opening, hence requiring manual aggregation of singulaer datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In This Section, We have 11 data sources. There are :\n",
    "1. 10 driver datasets \n",
    "2. Label datasets\n",
    "\n",
    "Steps of our data processing are :\n",
    "1. Import all datasets\n",
    "2. Concate 10 driver datasets to become a single main dataset as dataframe\n",
    "3. Get Dataset Information (Like some missing value, size of table, metadata, etc)\n",
    "4. Do some Feature Engineering that adds new features to the dataset\n",
    "5. Aggregate the dataset by 'bookingID' variable with mean (average)\n",
    "6. Merge 'label' dataset with the main dataset\n",
    "7. Clean the Dataset\n",
    "    1. Avoid redundant values\n",
    "    2. Missing Value Checking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Starts From Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing All Modules (Pre-Step 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# GridSearchCV is like k-fold.They work together,training and testing sets (70/30)\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "# Does all mathematical operations. So, it does ALL feature engineering as well.\n",
    "import pandas as pd\n",
    "# For the dataframe\n",
    "import matplotlib.pyplot as plt\n",
    "# For plotting charts\n",
    "import seaborn as sns\n",
    "# For plotting charts\n",
    "from sklearn.metrics import classification_report\n",
    "# Brief reporting tool for classification. See classification report below\n",
    "from imblearn.pipeline import Pipeline\n",
    "# More efficient than GridSearch CV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import make_scorer,accuracy_score,roc_auc_score,precision_score,recall_score,f1_score,log_loss\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing All Datasets (Step 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('part-00000-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv')\n",
    "data2 = pd.read_csv('part-00001-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv')\n",
    "data3 = pd.read_csv('part-00002-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv')\n",
    "data4 = pd.read_csv('part-00003-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv')\n",
    "data5 = pd.read_csv('part-00004-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv')\n",
    "data6 = pd.read_csv('part-00005-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv')\n",
    "data7 = pd.read_csv('part-00006-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv')\n",
    "data8 = pd.read_csv('part-00007-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv')\n",
    "data9 = pd.read_csv('part-00008-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv')\n",
    "data10 = pd.read_csv('part-00009-e6120af0-10c2-4248-97c4-81baf4304e5c-c000.csv')\n",
    "label = pd.read_csv('part-00000-e9445087-aa0a-433b-a7f6-7f4c19d78ad6-c000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating All Datasets (Step 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16135561 entries, 0 to 1613561\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Dtype  \n",
      "---  ------          -----  \n",
      " 0   bookingID       int64  \n",
      " 1   Accuracy        float64\n",
      " 2   Bearing         float64\n",
      " 3   acceleration_x  float64\n",
      " 4   acceleration_y  float64\n",
      " 5   acceleration_z  float64\n",
      " 6   gyro_x          float64\n",
      " 7   gyro_y          float64\n",
      " 8   gyro_z          float64\n",
      " 9   second          float64\n",
      " 10  Speed           float64\n",
      "dtypes: float64(10), int64(1)\n",
      "memory usage: 1.4 GB\n"
     ]
    }
   ],
   "source": [
    "data=pd.concat([data1,data2,data3,data4,data5,data6,data7,data8,data9,data10],0)\n",
    "data.info()\n",
    "# data.head is useful for quickly testing if your object has the right type of data in it.\n",
    "# data.info prints information about a DataFrame including the index dtype and columns, non-null values and memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookingID</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Bearing</th>\n",
       "      <th>acceleration_x</th>\n",
       "      <th>acceleration_y</th>\n",
       "      <th>acceleration_z</th>\n",
       "      <th>gyro_x</th>\n",
       "      <th>gyro_y</th>\n",
       "      <th>gyro_z</th>\n",
       "      <th>second</th>\n",
       "      <th>Speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1202590843006</td>\n",
       "      <td>3.000</td>\n",
       "      <td>353.0</td>\n",
       "      <td>1.228867</td>\n",
       "      <td>8.900100</td>\n",
       "      <td>3.986968</td>\n",
       "      <td>0.008221</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>-0.009966</td>\n",
       "      <td>1362.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>274877907034</td>\n",
       "      <td>9.293</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.032775</td>\n",
       "      <td>8.659933</td>\n",
       "      <td>4.737300</td>\n",
       "      <td>0.024629</td>\n",
       "      <td>0.004028</td>\n",
       "      <td>-0.010858</td>\n",
       "      <td>257.0</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>884763263056</td>\n",
       "      <td>3.000</td>\n",
       "      <td>189.0</td>\n",
       "      <td>1.139675</td>\n",
       "      <td>9.545974</td>\n",
       "      <td>1.951334</td>\n",
       "      <td>-0.006899</td>\n",
       "      <td>-0.015080</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>973.0</td>\n",
       "      <td>0.667059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1073741824054</td>\n",
       "      <td>3.900</td>\n",
       "      <td>126.0</td>\n",
       "      <td>3.871543</td>\n",
       "      <td>10.386364</td>\n",
       "      <td>-0.136474</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>-0.339601</td>\n",
       "      <td>-0.017956</td>\n",
       "      <td>902.0</td>\n",
       "      <td>7.913285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1056561954943</td>\n",
       "      <td>3.900</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.112882</td>\n",
       "      <td>10.550960</td>\n",
       "      <td>-1.560110</td>\n",
       "      <td>0.130568</td>\n",
       "      <td>-0.061697</td>\n",
       "      <td>0.161530</td>\n",
       "      <td>820.0</td>\n",
       "      <td>20.419409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bookingID  Accuracy  Bearing  acceleration_x  acceleration_y  \\\n",
       "0  1202590843006     3.000    353.0        1.228867        8.900100   \n",
       "1   274877907034     9.293     17.0        0.032775        8.659933   \n",
       "2   884763263056     3.000    189.0        1.139675        9.545974   \n",
       "3  1073741824054     3.900    126.0        3.871543       10.386364   \n",
       "4  1056561954943     3.900     50.0       -0.112882       10.550960   \n",
       "\n",
       "   acceleration_z    gyro_x    gyro_y    gyro_z  second      Speed  \n",
       "0        3.986968  0.008221  0.002269 -0.009966  1362.0   0.000000  \n",
       "1        4.737300  0.024629  0.004028 -0.010858   257.0   0.190000  \n",
       "2        1.951334 -0.006899 -0.015080  0.001122   973.0   0.667059  \n",
       "3       -0.136474  0.001344 -0.339601 -0.017956   902.0   7.913285  \n",
       "4       -1.560110  0.130568 -0.061697  0.161530   820.0  20.419409  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n",
    "# data.head is useful for quickly testing if your object has the right type of data in it. Same as before, but this is after the concat and nicer formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Distance\n",
    "data['distance']=data['Speed']*data['second']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this feature, I multiply 'acceleration' with 'second' to produce feature 'second'.\n",
    "I use Euclid distance to determine the interaction from the Feature Speed. \n",
    "\n",
    "\n",
    "I use this method to add new faeture because this feature contain direction/vector like (x,y,z)\n",
    "Euclidean distance between two points in Euclidean space is a number, the length of a line segment between the two points.\n",
    "\n",
    "(This is basically just replicating Euclid distance formula. Nothing revolutionary here.See the formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Speed\n",
    "data['speed_x']=data['acceleration_x']*data['second']\n",
    "data['speed_y']=data['acceleration_y']*data['second']\n",
    "data['speed_z']=data['acceleration_z']*data['second']\n",
    "data['speed_xy']=np.sqrt(data['speed_x']**2+data['speed_y']**2)\n",
    "data['speed_xz']=np.sqrt(data['speed_x']**2+data['speed_z']**2)\n",
    "data['speed_yz']=np.sqrt(data['speed_z']**2+data['speed_y']**2)\n",
    "data['speed_xyz']=np.sqrt(data['speed_x']**2+data['speed_y']**2+data['speed_z']**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Radian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this feature, I multiply 'gyro' with 'second' to produce feature 'second'\n",
    "I use Euclid distance to determine the interaction from the Feature Radian. \n",
    "I use this method to add new faeture because this feature contain direction/vector like (x,y,z)\n",
    "\n",
    "Same deal as before (speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###radian\n",
    "data['rad_x']=data['gyro_x']*data['second']\n",
    "data['rad_y']=data['gyro_y']*data['second']\n",
    "data['rad_z']=data['gyro_z']*data['second']\n",
    "data['rad_xy']=np.sqrt(data['rad_x']**2+data['rad_y']**2)\n",
    "data['rad_xz']=np.sqrt(data['rad_x']**2+data['rad_z']**2)\n",
    "data['rad_yz']=np.sqrt(data['rad_z']**2+data['rad_y']**2)\n",
    "data['rad_xyz']=np.sqrt(data['rad_x']**2+data['rad_y']**2+data['rad_z']**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I make the interaction variable with all the combination of the feature 'Acceleration' based on the vector theorem. \n",
    "I use Euclid Method to combine the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Acceleration\n",
    "data['acc_xy']=np.sqrt(data['acceleration_x']**2+data['acceleration_y']**2)\n",
    "data['acc_xz']=np.sqrt(data['acceleration_x']**2+data['acceleration_z']**2)\n",
    "data['acc_yz']=np.sqrt(data['acceleration_z']**2+data['acceleration_y']**2)\n",
    "data['acc_xyz']=np.sqrt(data['acceleration_x']**2+data['acceleration_y']**2+data['acceleration_z']**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Gyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I make the interaction variable with all the combination of the feature 'Gyro' based on the vector theorem. \n",
    "I use Euclid Method to combine the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Gyro\n",
    "data['gyro_xy']=np.sqrt(data['gyro_x']**2+data['gyro_y']**2)\n",
    "data['gyro_xz']=np.sqrt(data['gyro_x']**2+data['gyro_z']**2)\n",
    "data['gyro_yz']=np.sqrt(data['gyro_z']**2+data['gyro_y']**2)\n",
    "data['gyro_xyz']=np.sqrt(data['gyro_x']**2+data['gyro_y']**2+data['gyro_z']**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Interaction\n",
    "data['acc_gyro_x']=data['acceleration_x']*data['gyro_x']\n",
    "data['acc_gyro_y']=data['acceleration_y']*data['gyro_y']\n",
    "data['acc_gyro_z']=data['acceleration_z']*data['gyro_z']\n",
    "data['acc_gyro_xy']=np.sqrt(data['acc_gyro_x']**2+data['acc_gyro_y']**2)\n",
    "data['acc_gyro_xz']=np.sqrt(data['acc_gyro_x']**2+data['acc_gyro_z']**2)\n",
    "data['acc_gyro_yz']=np.sqrt(data['acc_gyro_z']**2+data['acc_gyro_y']**2)\n",
    "data['acc_gyro_xyz']=np.sqrt(data['acc_gyro_x']**2+data['acc_gyro_y']**2+data['acc_gyro_z']**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I aggregate all the 'bookingID' Feature by count so we know how many calls the driver got.\n",
    "Also known as \"how many times this bookingID appears\"\n",
    "\n",
    "Eg: We have booking ID 101. How many times does this appear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=data['bookingID'].value_counts()\n",
    "ID=pd.DataFrame(a)\n",
    "ID['id']=ID.index\n",
    "ID['count']=ID['bookingID']\n",
    "#ID.head()\n",
    "Count_df=pd.DataFrame()\n",
    "# Convert it into dataframe and call it ID. BookingID = count (ID)\n",
    "Count_df['bookingID']=ID['id']\n",
    "Count_df['count']=ID['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookingID</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>438086664371</th>\n",
       "      <td>438086664371</td>\n",
       "      <td>7561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374389534819</th>\n",
       "      <td>1374389534819</td>\n",
       "      <td>4499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34359738469</th>\n",
       "      <td>34359738469</td>\n",
       "      <td>4302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108101562533</th>\n",
       "      <td>1108101562533</td>\n",
       "      <td>3925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747324309632</th>\n",
       "      <td>747324309632</td>\n",
       "      <td>3674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537598292022</th>\n",
       "      <td>1537598292022</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180388626478</th>\n",
       "      <td>180388626478</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317827579936</th>\n",
       "      <td>317827579936</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472446402608</th>\n",
       "      <td>472446402608</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571958030400</th>\n",
       "      <td>1571958030400</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   bookingID  count\n",
       "438086664371    438086664371   7561\n",
       "1374389534819  1374389534819   4499\n",
       "34359738469      34359738469   4302\n",
       "1108101562533  1108101562533   3925\n",
       "747324309632    747324309632   3674\n",
       "...                      ...    ...\n",
       "1537598292022  1537598292022    120\n",
       "180388626478    180388626478    120\n",
       "317827579936    317827579936    120\n",
       "472446402608    472446402608    120\n",
       "1571958030400  1571958030400    120\n",
       "\n",
       "[20000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Count_df\n",
    "# Shows you the count of booking ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Aggregation by Mean for BookingID and Merge the aggregation data with label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable=['Accuracy', 'Bearing', 'acceleration_x', 'acceleration_y',\n",
    "       'acceleration_z', 'gyro_x', 'gyro_y', 'gyro_z', 'second', 'Speed',\n",
    "       'distance', 'speed_x', 'speed_y', 'speed_z', 'speed_xy', 'speed_xz',\n",
    "       'speed_yz', 'speed_xyz', 'rad_x', 'rad_y', 'rad_z', 'rad_xy', 'rad_xz',\n",
    "       'rad_yz', 'rad_xyz', 'acc_xy', 'acc_xz', 'acc_yz', 'acc_xyz', 'gyro_xy',\n",
    "       'gyro_xz', 'gyro_yz', 'gyro_xyz', 'acc_gyro_x', 'acc_gyro_y',\n",
    "        'acc_gyro_z', 'acc_gyro_xy', 'acc_gyro_xz', 'acc_gyro_yz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20000 entries, 0 to 19999\n",
      "Data columns (total 40 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   bookingID       20000 non-null  int64  \n",
      " 1   Accuracy        20000 non-null  float64\n",
      " 2   Bearing         20000 non-null  float64\n",
      " 3   acceleration_x  20000 non-null  float64\n",
      " 4   acceleration_y  20000 non-null  float64\n",
      " 5   acceleration_z  20000 non-null  float64\n",
      " 6   gyro_x          20000 non-null  float64\n",
      " 7   gyro_y          20000 non-null  float64\n",
      " 8   gyro_z          20000 non-null  float64\n",
      " 9   second          20000 non-null  float64\n",
      " 10  Speed           20000 non-null  float64\n",
      " 11  distance        20000 non-null  float64\n",
      " 12  speed_x         20000 non-null  float64\n",
      " 13  speed_y         20000 non-null  float64\n",
      " 14  speed_z         20000 non-null  float64\n",
      " 15  speed_xy        20000 non-null  float64\n",
      " 16  speed_xz        20000 non-null  float64\n",
      " 17  speed_yz        20000 non-null  float64\n",
      " 18  speed_xyz       20000 non-null  float64\n",
      " 19  rad_x           20000 non-null  float64\n",
      " 20  rad_y           20000 non-null  float64\n",
      " 21  rad_z           20000 non-null  float64\n",
      " 22  rad_xy          20000 non-null  float64\n",
      " 23  rad_xz          20000 non-null  float64\n",
      " 24  rad_yz          20000 non-null  float64\n",
      " 25  rad_xyz         20000 non-null  float64\n",
      " 26  acc_xy          20000 non-null  float64\n",
      " 27  acc_xz          20000 non-null  float64\n",
      " 28  acc_yz          20000 non-null  float64\n",
      " 29  acc_xyz         20000 non-null  float64\n",
      " 30  gyro_xy         20000 non-null  float64\n",
      " 31  gyro_xz         20000 non-null  float64\n",
      " 32  gyro_yz         20000 non-null  float64\n",
      " 33  gyro_xyz        20000 non-null  float64\n",
      " 34  acc_gyro_x      20000 non-null  float64\n",
      " 35  acc_gyro_y      20000 non-null  float64\n",
      " 36  acc_gyro_z      20000 non-null  float64\n",
      " 37  acc_gyro_xy     20000 non-null  float64\n",
      " 38  acc_gyro_xz     20000 non-null  float64\n",
      " 39  acc_gyro_yz     20000 non-null  float64\n",
      "dtypes: float64(39), int64(1)\n",
      "memory usage: 6.3 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20018 entries, 0 to 20017\n",
      "Data columns (total 41 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   bookingID       20018 non-null  int64  \n",
      " 1   Accuracy        20018 non-null  float64\n",
      " 2   Bearing         20018 non-null  float64\n",
      " 3   acceleration_x  20018 non-null  float64\n",
      " 4   acceleration_y  20018 non-null  float64\n",
      " 5   acceleration_z  20018 non-null  float64\n",
      " 6   gyro_x          20018 non-null  float64\n",
      " 7   gyro_y          20018 non-null  float64\n",
      " 8   gyro_z          20018 non-null  float64\n",
      " 9   second          20018 non-null  float64\n",
      " 10  Speed           20018 non-null  float64\n",
      " 11  distance        20018 non-null  float64\n",
      " 12  speed_x         20018 non-null  float64\n",
      " 13  speed_y         20018 non-null  float64\n",
      " 14  speed_z         20018 non-null  float64\n",
      " 15  speed_xy        20018 non-null  float64\n",
      " 16  speed_xz        20018 non-null  float64\n",
      " 17  speed_yz        20018 non-null  float64\n",
      " 18  speed_xyz       20018 non-null  float64\n",
      " 19  rad_x           20018 non-null  float64\n",
      " 20  rad_y           20018 non-null  float64\n",
      " 21  rad_z           20018 non-null  float64\n",
      " 22  rad_xy          20018 non-null  float64\n",
      " 23  rad_xz          20018 non-null  float64\n",
      " 24  rad_yz          20018 non-null  float64\n",
      " 25  rad_xyz         20018 non-null  float64\n",
      " 26  acc_xy          20018 non-null  float64\n",
      " 27  acc_xz          20018 non-null  float64\n",
      " 28  acc_yz          20018 non-null  float64\n",
      " 29  acc_xyz         20018 non-null  float64\n",
      " 30  gyro_xy         20018 non-null  float64\n",
      " 31  gyro_xz         20018 non-null  float64\n",
      " 32  gyro_yz         20018 non-null  float64\n",
      " 33  gyro_xyz        20018 non-null  float64\n",
      " 34  acc_gyro_x      20018 non-null  float64\n",
      " 35  acc_gyro_y      20018 non-null  float64\n",
      " 36  acc_gyro_z      20018 non-null  float64\n",
      " 37  acc_gyro_xy     20018 non-null  float64\n",
      " 38  acc_gyro_xz     20018 non-null  float64\n",
      " 39  acc_gyro_yz     20018 non-null  float64\n",
      " 40  label           20018 non-null  int64  \n",
      "dtypes: float64(39), int64(2)\n",
      "memory usage: 6.4 MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookingID</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Bearing</th>\n",
       "      <th>acceleration_x</th>\n",
       "      <th>acceleration_y</th>\n",
       "      <th>acceleration_z</th>\n",
       "      <th>gyro_x</th>\n",
       "      <th>gyro_y</th>\n",
       "      <th>gyro_z</th>\n",
       "      <th>second</th>\n",
       "      <th>...</th>\n",
       "      <th>gyro_xz</th>\n",
       "      <th>gyro_yz</th>\n",
       "      <th>gyro_xyz</th>\n",
       "      <th>acc_gyro_x</th>\n",
       "      <th>acc_gyro_y</th>\n",
       "      <th>acc_gyro_z</th>\n",
       "      <th>acc_gyro_xy</th>\n",
       "      <th>acc_gyro_xz</th>\n",
       "      <th>acc_gyro_yz</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10.165339</td>\n",
       "      <td>176.526099</td>\n",
       "      <td>-0.711264</td>\n",
       "      <td>-9.613822</td>\n",
       "      <td>-1.619658</td>\n",
       "      <td>0.003328</td>\n",
       "      <td>-0.006118</td>\n",
       "      <td>-0.004188</td>\n",
       "      <td>903.526892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068242</td>\n",
       "      <td>0.082938</td>\n",
       "      <td>0.100772</td>\n",
       "      <td>-0.011891</td>\n",
       "      <td>0.067631</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>0.633826</td>\n",
       "      <td>0.105596</td>\n",
       "      <td>0.645055</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.718763</td>\n",
       "      <td>124.198590</td>\n",
       "      <td>-0.525406</td>\n",
       "      <td>9.532086</td>\n",
       "      <td>-2.198999</td>\n",
       "      <td>-0.002467</td>\n",
       "      <td>-0.007540</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>581.175088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.060189</td>\n",
       "      <td>0.066187</td>\n",
       "      <td>0.002711</td>\n",
       "      <td>-0.070063</td>\n",
       "      <td>-0.001414</td>\n",
       "      <td>0.497354</td>\n",
       "      <td>0.056595</td>\n",
       "      <td>0.503368</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.930626</td>\n",
       "      <td>173.794872</td>\n",
       "      <td>0.306786</td>\n",
       "      <td>9.843183</td>\n",
       "      <td>0.139347</td>\n",
       "      <td>0.006458</td>\n",
       "      <td>-0.012861</td>\n",
       "      <td>0.002597</td>\n",
       "      <td>339.441026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041984</td>\n",
       "      <td>0.082883</td>\n",
       "      <td>0.097433</td>\n",
       "      <td>-0.000493</td>\n",
       "      <td>-0.124528</td>\n",
       "      <td>-0.003984</td>\n",
       "      <td>0.718235</td>\n",
       "      <td>0.038764</td>\n",
       "      <td>0.716790</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>151.807013</td>\n",
       "      <td>-0.365117</td>\n",
       "      <td>-9.406439</td>\n",
       "      <td>-2.613639</td>\n",
       "      <td>-0.022884</td>\n",
       "      <td>0.023232</td>\n",
       "      <td>-0.000376</td>\n",
       "      <td>547.495430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062106</td>\n",
       "      <td>0.098309</td>\n",
       "      <td>0.108875</td>\n",
       "      <td>0.010705</td>\n",
       "      <td>-0.224052</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.743528</td>\n",
       "      <td>0.125491</td>\n",
       "      <td>0.767880</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4.586721</td>\n",
       "      <td>197.812785</td>\n",
       "      <td>0.490616</td>\n",
       "      <td>9.538043</td>\n",
       "      <td>2.355059</td>\n",
       "      <td>0.003877</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>547.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057885</td>\n",
       "      <td>0.073102</td>\n",
       "      <td>0.089589</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.007564</td>\n",
       "      <td>0.005758</td>\n",
       "      <td>0.549155</td>\n",
       "      <td>0.100458</td>\n",
       "      <td>0.560269</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bookingID   Accuracy     Bearing  acceleration_x  acceleration_y  \\\n",
       "0          0  10.165339  176.526099       -0.711264       -9.613822   \n",
       "1          1   3.718763  124.198590       -0.525406        9.532086   \n",
       "2          2   3.930626  173.794872        0.306786        9.843183   \n",
       "3          4  10.000000  151.807013       -0.365117       -9.406439   \n",
       "4          6   4.586721  197.812785        0.490616        9.538043   \n",
       "\n",
       "   acceleration_z    gyro_x    gyro_y    gyro_z      second  ...   gyro_xz  \\\n",
       "0       -1.619658  0.003328 -0.006118 -0.004188  903.526892  ...  0.068242   \n",
       "1       -2.198999 -0.002467 -0.007540  0.000405  581.175088  ...  0.032787   \n",
       "2        0.139347  0.006458 -0.012861  0.002597  339.441026  ...  0.041984   \n",
       "3       -2.613639 -0.022884  0.023232 -0.000376  547.495430  ...  0.062106   \n",
       "4        2.355059  0.003877  0.000436  0.002930  547.000000  ...  0.057885   \n",
       "\n",
       "    gyro_yz  gyro_xyz  acc_gyro_x  acc_gyro_y  acc_gyro_z  acc_gyro_xy  \\\n",
       "0  0.082938  0.100772   -0.011891    0.067631    0.001921     0.633826   \n",
       "1  0.060189  0.066187    0.002711   -0.070063   -0.001414     0.497354   \n",
       "2  0.082883  0.097433   -0.000493   -0.124528   -0.003984     0.718235   \n",
       "3  0.098309  0.108875    0.010705   -0.224052    0.000394     0.743528   \n",
       "4  0.073102  0.089589    0.002100    0.007564    0.005758     0.549155   \n",
       "\n",
       "   acc_gyro_xz  acc_gyro_yz  label  \n",
       "0     0.105596     0.645055      0  \n",
       "1     0.056595     0.503368      1  \n",
       "2     0.038764     0.716790      1  \n",
       "3     0.125491     0.767880      1  \n",
       "4     0.100458     0.560269      0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataAg=data.groupby('bookingID', as_index=False)[variable].mean()\n",
    "# Note: This takes all the grouped by bookingID values and gets the mean (average) of them, so you don't end up with loads of values.\n",
    "dataAg.info()\n",
    "df_main = pd.merge(dataAg,label,how='left',left_on = 'bookingID',right_on = 'bookingID')\n",
    "\n",
    "print(df_main.info())\n",
    "df_main.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_main.drop(['label'],1)\n",
    "df_main = df_main.drop(['bookingID'],axis=1)\n",
    "df_main = pd.get_dummies(df_main)\n",
    "# All the dataAg are in X. It just drops the label column from df.main, and the label column goes to y.\n",
    "# get_dummies will convert all the variables into binary (0/1) for Machine Learning\n",
    "y = df_main['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the data balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.750175\n",
       "1    0.249825\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main['label'].value_counts()\n",
    "df_main['label'].value_counts(normalize = True)\n",
    "# Normalizing the data means converting all values fell between 0 and 1 BUT were NOT 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is unbalanced, so I use sampling to avoid the affect from imbalancing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values\n",
    "# Extract all values and convert into series object (in an array format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE MODELLING FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we build this function we use some method :\n",
    "1. Training and testing split\n",
    "2. Stratified 10-Fold Cross Validation each method classification in training data\n",
    "3. Balancing data when we build a model, but in validation fold is similar like original data\n",
    "4. Evaluate every model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import make_scorer,accuracy_score,roc_auc_score,precision_score,recall_score,f1_score,log_loss\n",
    "\n",
    "def geometric_mean_pres_rec(y_true,y_pred):\n",
    "    score = precision_score(y_true,y_pred)*recall_score(y_true,y_pred)\n",
    "    return(score)\n",
    "# This gives you a score, which is the model's performance (in terms of precision and recall)\n",
    "\n",
    "# model_classification, when called runs all these classification methods (all 7)\n",
    "def model_classification(verbose):\n",
    "    # simple classification\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.naive_bayes import BernoulliNB\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    \n",
    "    # hard classification\n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Set Simple Model\")\n",
    "    logreg = LogisticRegression(penalty='l2')\n",
    "    knn = KNeighborsClassifier()\n",
    "    nb = BernoulliNB()\n",
    "    dt = DecisionTreeClassifier()\n",
    "    \n",
    "    if verbose:print(\"Set Hard Model\\n\")\n",
    "    rf = RandomForestClassifier(random_state = 77)\n",
    "    gbc = GradientBoostingClassifier(random_state=77 )\n",
    "    xgb = XGBClassifier(random_state=77)\n",
    "    \n",
    "    model = [logreg,knn,nb,dt,rf,gbc,xgb]\n",
    "    method_name = ['Logistic Regression',\n",
    "                  'K-Nearest Neighbor',\n",
    "                  'Naive Bayes',\n",
    "                  'Decision Tree',\n",
    "                  'Random Forest',\n",
    "                  'Gradient Boosting Classifier',\n",
    "                  'Extreme Gradient Boosting']\n",
    "    return [model,method_name]\n",
    "    \n",
    "def pipe_imbalance(X,y,imb = RandomUnderSampler(),verbose = False):\n",
    "# This takes two arguments (the features and the label from earlier) and run all these things in the column on them. \n",
    "# Basically, it just evaluates all of them on these metrics    \n",
    "    df_eval = pd.DataFrame(columns = ['Model',\n",
    "                                    'Accuracy',\n",
    "                                    'Precision',\n",
    "                                    'Recall',\n",
    "                                    'AUC',\n",
    "                                    'F1_score',\n",
    "                                    'Log_loss',\n",
    "                                    'Geometric_Mean',\n",
    "                                    'Geometric_Mean_Precision_Recall',\n",
    "                                    'Time'])\n",
    "\n",
    "    if verbose: print(\"Split Training and Testing\\n\")\n",
    "    X_train, X_test,y_train,y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size = 0.3,\n",
    "                                                        stratify = y,\n",
    "                                                        #random_state = 123\n",
    "                                                     )\n",
    "\n",
    "    if verbose: print('Import Classification Method\\n')\n",
    "    \n",
    "    list_model = model_classification(True)\n",
    "    df_eval['Model']=list_model[1]\n",
    "\n",
    "    if verbose: print('Building Pipeline\\n')\n",
    "    pipe = Pipeline([('imb',imb),('classifier',LogisticRegression())])\n",
    "\n",
    "    if verbose: print('Defining Params and Scoring\\n')\n",
    "    params = {'classifier': list_model[0]}\n",
    "    scorers = {'accuracy':make_scorer(accuracy_score),\n",
    "               'precision':make_scorer(precision_score),\n",
    "               'recall':make_scorer(recall_score),\n",
    "               'roc_auc':make_scorer(roc_auc_score),\n",
    "               'f1':make_scorer(f1_score),\n",
    "               'neg_log_loss':'neg_log_loss',\n",
    "               'gm':make_scorer(geometric_mean_score),\n",
    "               'gmpr':make_scorer(geometric_mean_pres_rec)}\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=10,random_state = 7)\n",
    "    \n",
    "    grid = GridSearchCV(estimator = pipe,\n",
    "                    param_grid = params,\n",
    "                    scoring = scorers,\n",
    "                    refit = 'accuracy',\n",
    "                    cv = skf)\n",
    "    grid.fit(X_train,y_train)\n",
    "    \n",
    "    df_eval['Accuracy']  = grid.cv_results_['mean_test_accuracy']\n",
    "    df_eval['Precision'] = grid.cv_results_['mean_test_precision']\n",
    "    df_eval['Recall'] = grid.cv_results_['mean_test_recall']\n",
    "    df_eval['AUC'] = grid.cv_results_['mean_test_roc_auc']\n",
    "    df_eval['F1_score'] = grid.cv_results_['mean_test_f1']\n",
    "    df_eval['Log_loss'] = grid.cv_results_['mean_test_neg_log_loss']\n",
    "    df_eval['Geometric_Mean'] = grid.cv_results_['mean_test_gm']\n",
    "    df_eval['Geometric_Mean_Precision_Recall'] = grid.cv_results_['mean_test_gmpr']\n",
    "    df_eval['Time'] = grid.cv_results_['mean_fit_time']\n",
    "    \n",
    "    return [grid,df_eval,X_test,y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Training and Testing\n",
      "\n",
      "Import Classification Method\n",
      "\n",
      "Set Simple Model\n",
      "Set Hard Model\n",
      "\n",
      "Building Pipeline\n",
      "\n",
      "Defining Params and Scoring\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_under = pipe_imbalance(X,y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>AUC</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Log_loss</th>\n",
       "      <th>Geometric_Mean</th>\n",
       "      <th>Geometric_Mean_Precision_Recall</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.249857</td>\n",
       "      <td>0.249857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.399817</td>\n",
       "      <td>-0.703113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249857</td>\n",
       "      <td>0.044190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K-Nearest Neighbor</td>\n",
       "      <td>0.653369</td>\n",
       "      <td>0.376148</td>\n",
       "      <td>0.586405</td>\n",
       "      <td>0.631039</td>\n",
       "      <td>0.458130</td>\n",
       "      <td>-1.337362</td>\n",
       "      <td>0.629260</td>\n",
       "      <td>0.220570</td>\n",
       "      <td>0.069326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.508423</td>\n",
       "      <td>0.252909</td>\n",
       "      <td>0.494450</td>\n",
       "      <td>0.503766</td>\n",
       "      <td>0.334137</td>\n",
       "      <td>-0.694608</td>\n",
       "      <td>0.501920</td>\n",
       "      <td>0.125272</td>\n",
       "      <td>0.018042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.576507</td>\n",
       "      <td>0.314033</td>\n",
       "      <td>0.587264</td>\n",
       "      <td>0.580094</td>\n",
       "      <td>0.409172</td>\n",
       "      <td>-14.624808</td>\n",
       "      <td>0.579796</td>\n",
       "      <td>0.184789</td>\n",
       "      <td>0.303095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.659079</td>\n",
       "      <td>0.383875</td>\n",
       "      <td>0.598977</td>\n",
       "      <td>0.639039</td>\n",
       "      <td>0.467650</td>\n",
       "      <td>-0.619364</td>\n",
       "      <td>0.637518</td>\n",
       "      <td>0.230305</td>\n",
       "      <td>3.306624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.670141</td>\n",
       "      <td>0.396895</td>\n",
       "      <td>0.615548</td>\n",
       "      <td>0.651938</td>\n",
       "      <td>0.482390</td>\n",
       "      <td>-0.598009</td>\n",
       "      <td>0.650599</td>\n",
       "      <td>0.244461</td>\n",
       "      <td>6.997249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Extreme Gradient Boosting</td>\n",
       "      <td>0.645875</td>\n",
       "      <td>0.374840</td>\n",
       "      <td>0.624679</td>\n",
       "      <td>0.638807</td>\n",
       "      <td>0.468460</td>\n",
       "      <td>-0.665347</td>\n",
       "      <td>0.638488</td>\n",
       "      <td>0.234452</td>\n",
       "      <td>1.867273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy  Precision    Recall       AUC  \\\n",
       "0           Logistic Regression  0.249857   0.249857  1.000000  0.500000   \n",
       "1            K-Nearest Neighbor  0.653369   0.376148  0.586405  0.631039   \n",
       "2                   Naive Bayes  0.508423   0.252909  0.494450  0.503766   \n",
       "3                 Decision Tree  0.576507   0.314033  0.587264  0.580094   \n",
       "4                 Random Forest  0.659079   0.383875  0.598977  0.639039   \n",
       "5  Gradient Boosting Classifier  0.670141   0.396895  0.615548  0.651938   \n",
       "6     Extreme Gradient Boosting  0.645875   0.374840  0.624679  0.638807   \n",
       "\n",
       "   F1_score   Log_loss  Geometric_Mean  Geometric_Mean_Precision_Recall  \\\n",
       "0  0.399817  -0.703113        0.000000                         0.249857   \n",
       "1  0.458130  -1.337362        0.629260                         0.220570   \n",
       "2  0.334137  -0.694608        0.501920                         0.125272   \n",
       "3  0.409172 -14.624808        0.579796                         0.184789   \n",
       "4  0.467650  -0.619364        0.637518                         0.230305   \n",
       "5  0.482390  -0.598009        0.650599                         0.244461   \n",
       "6  0.468460  -0.665347        0.638488                         0.234452   \n",
       "\n",
       "       Time  \n",
       "0  0.044190  \n",
       "1  0.069326  \n",
       "2  0.018042  \n",
       "3  0.303095  \n",
       "4  3.306624  \n",
       "5  6.997249  \n",
       "6  1.867273  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_under[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Training and Testing\n",
      "\n",
      "Import Classification Method\n",
      "\n",
      "Set Simple Model\n",
      "Set Hard Model\n",
      "\n",
      "Building Pipeline\n",
      "\n",
      "Defining Params and Scoring\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_smote = pipe_imbalance(X,y,imb=SMOTE(k_neighbors=11), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the model comparison dataframe, we can see that Gradient boosting Classifier with Random Under Sampling is the best Model because It has good evaluation metrics and the highest AUC Score. After we get the model, We tune the Gradient boosting Classifier parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will optimize our model by tuning our hyperparameter. The model that will be tuned is Gradient Boosting Classifier. We use this model beacuse it has good evaluation metrics among the other model. The parameter that will be tuned are:\n",
    "1. Number of Tree (n_estimators)\n",
    "2. Maximum of Tree (max_depth)\n",
    "3. Minimum Sample in Leaf (min_samples_leaf)\n",
    "4. Minimum Sample in each split (min_samples_split)\n",
    "5. Number of Data for Bootstrap (subsample)\n",
    "6. Learning Rate (learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold,GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVAL METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library for Evaluation Metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import make_scorer,precision_score\n",
    "from sklearn.metrics import recall_score,accuracy_score,f1_score\n",
    "\n",
    "# library for calculate the cross validation score\n",
    "from sklearn.model_selection import cross_val_score\n",
    " \n",
    "# library for Stratified Cross Validation\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geometri_score(y_true,y_pred):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    spe = confusion_matrix(y_true,y_pred)[0,0]/(confusion_matrix(y_true,y_pred)[0,0]+confusion_matrix(y_true,y_pred)[0,1])\n",
    "    sen = recall_score(y_true,y_pred)\n",
    "    acc = accuracy_score(y_true,y_pred)\n",
    "    eval_baru = (spe*sen*acc)**(1/3)\n",
    "    return eval_baru\n",
    "\n",
    "geometri = make_scorer(geometri_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score), 'accuracy_score': make_scorer(accuracy_score), 'auc': make_scorer(auc), 'geometric_mean_score': make_scorer(geometri_score)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer,precision_score,recall_score,accuracy_score,auc\n",
    "scorers = {\n",
    "    'precision_score':make_scorer(precision_score),\n",
    "    'recall_score':make_scorer(recall_score),\n",
    "    'accuracy_score':make_scorer(accuracy_score),\n",
    "    'auc':make_scorer(auc),\n",
    "    'geometric_mean_score': make_scorer(geometri_score)\n",
    "}\n",
    "print(scorers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop Evaluation Function\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,random_state=10)\n",
    "\n",
    "def eval_cv(alg,X,y):\n",
    "    print(\"Accuracy cv : \\n\",\n",
    "          cross_val_score(alg,X,y,scoring=\"accuracy\",cv=skf),\"\\n\",\n",
    "          cross_val_score(alg,X,y,scoring=\"accuracy\",cv=skf).mean(),\"\\n\")\n",
    "    print(\"Recall cv : \\n\",\n",
    "          cross_val_score(alg,X,y,scoring=\"recall\",cv=skf),\"\\n\",\n",
    "          cross_val_score(alg,X,y,scoring=\"recall\",cv=skf).mean(),\"\\n\")\n",
    "    print(\"Precision cv : \\n\",\n",
    "          cross_val_score(alg,X,y,scoring=\"precision\",cv=skf),\"\\n\",\n",
    "          cross_val_score(alg,X,y,scoring=\"precision\",cv=skf).mean(),\"\\n\")\n",
    "    print(\"f1 cv : \\n\",\n",
    "          cross_val_score(alg,X,y,scoring=\"f1\",cv=skf),\"\\n\",\n",
    "          cross_val_score(alg,X,y,scoring=\"f1\",cv=skf).mean(),\"\\n\")\n",
    "\n",
    "    \n",
    "def eval(alg,X,y):\n",
    "    alg.fit(X_train,y_train)\n",
    "    print(\"Accuracy:\\n\",\n",
    "          accuracy_score(y,alg.predict(X)),\"\\n\")\n",
    "    print(\"Confusion matrix test:\\n\",\n",
    "          confusion_matrix(y,alg.predict(X)),\"\\n\")\n",
    "    print(\"Classification Report test:\\n\",\n",
    "          classification_report(y,alg.predict(X)),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test Split and Modelling (Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from numpy import loadtxt\n",
    "from numpy import sort\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3, stratify=y)\n",
    "rus = RandomUnderSampler()\n",
    "X_sampled,y_sampled = rus.fit_sample(X_train,y_train)\n",
    "gbc = GradientBoostingClassifier()\n",
    "model=gbc.fit(X_sampled,y_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of GradientBoostingClassifier()>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_tune = GradientBoostingClassifier(criterion='friedman_mse',\n",
    "                    learning_rate=0.1,\n",
    "                    max_depth=3,\n",
    "                    max_features=None,\n",
    "                    min_samples_split=2,\n",
    "                    min_samples_leaf=1,\n",
    "                    n_estimators=100,\n",
    "                    subsample=1.0,                  \n",
    "                    random_state=123)\n",
    "\n",
    "pipe_gbc_tune = Pipeline([('imb',RandomUnderSampler(random_state=123)),('clf',gbc_tune)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy cv : \n",
      " [0.68355334 0.68283981 0.67380443 0.66702355 0.66952177] \n",
      " 0.6753485800749325 \n",
      "\n",
      "Recall cv : \n",
      " [0.60912981 0.59714286 0.61857143 0.64142857 0.61      ] \n",
      " 0.6152545343386998 \n",
      "\n",
      "Precision cv : \n",
      " [0.41057692 0.40780488 0.40092593 0.39699381 0.39537037] \n",
      " 0.4023343816417828 \n",
      "\n",
      "f1 cv : \n",
      " [0.49052269 0.48463768 0.48651685 0.49044238 0.47977528] \n",
      " 0.4863789770627228 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_cv(pipe_gbc_tune,X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning 1 (n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=123)\n",
    "\n",
    "gbc_tune = GradientBoostingClassifier(criterion='friedman_mse',\n",
    "                    learning_rate=0.1,\n",
    "                    max_depth=3,\n",
    "                    max_features=None,\n",
    "                    min_samples_split=2,\n",
    "                    min_samples_leaf=1,\n",
    "#                    n_estimators=100,\n",
    "                    subsample=1.0,                  \n",
    "                    random_state=123)\n",
    "\n",
    "select = SelectFromModel(gbc_tune,threshold='median')\n",
    "pipe_gbc = Pipeline([('rus',rus),('select',select),('clf',gbc_tune)])\n",
    "\n",
    "param_gb = {\n",
    "    'clf__n_estimators':[45,70,90,100,110,120,140]}\n",
    "# It will run for all these values. It decided 45 was the best.\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,random_state=123)\n",
    "\n",
    "gbc_grid = GridSearchCV(pipe_gbc,\n",
    "                       param_grid=param_gb,\n",
    "                       # Tuning Number 1/4\n",
    "                       refit = 'geometric',\n",
    "                       # Method Validation\n",
    "                       cv=skf)\n",
    "\n",
    "gbc_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gbc_grid.best_params_)\n",
    "print(gbc_grid.best_score_)\n",
    "eval(gbc_grid.best_estimator_,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning 2 ( Max_depth & Min_samples_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=123)\n",
    "\n",
    "# Tune  the models (minimum number of samples for max depth)\n",
    "gbc_tune2 = GradientBoostingClassifier(criterion='friedman_mse',\n",
    "                    learning_rate=0.1,\n",
    "#                    max_depth=3,\n",
    "                    max_features=None,\n",
    "#                    min_samples_split=2,\n",
    "                    min_samples_leaf=1,\n",
    "                    n_estimators=100,\n",
    "                    subsample=1.0,                  \n",
    "                    random_state=123)\n",
    "\n",
    "select = SelectFromModel(gbc_tune2,threshold='median')\n",
    "pipe_gbc2 = Pipeline([('rus',rus),('select',select),('clf',gbc_tune2)])\n",
    "\n",
    "# parameter tuning\n",
    "param_gb2 = {\n",
    "    'clf__max_depth':range(2,8,1),\n",
    "    'clf__min_samples_split':range(2,6,2)\n",
    "# Total 8 parameters. \n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,random_state=123)\n",
    "\n",
    "gbc_grid2 = GridSearchCV(pipe_gbc2,\n",
    "                       param_grid=param_gb2,\n",
    "                       # GridSearchCV tuning number 2\n",
    "                       refit = 'geometric',\n",
    "                       # Method Validation\n",
    "                       cv=skf)\n",
    "\n",
    "gbc_grid2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gbc_grid2.best_params_)\n",
    "print(gbc_grid2.best_score_)\n",
    "eval(gbc_grid2.best_estimator_,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning 3 (Min_samples_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=123)\n",
    "\n",
    "gbc_tune3 = GradientBoostingClassifier(criterion='friedman_mse',\n",
    "                    learning_rate=0.1,\n",
    "#                    max_depth=3,\n",
    "                    max_features=None,\n",
    "                    min_samples_split=2,\n",
    "#                    min_samples_leaf=1,\n",
    "                    n_estimators=100,\n",
    "                    subsample=1.0,                  \n",
    "                    random_state=123)\n",
    "\n",
    "select = SelectFromModel(gbc_tune3,threshold='median')\n",
    "pipe_gbc3 = Pipeline([('rus',rus),('select',select),('clf',gbc_tune3)])\n",
    "\n",
    "param_gb3 = {\n",
    "    'clf__max_depth':[3,4,5],\n",
    "    'clf__min_samples_leaf':[1,2,3,4,5,6]\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,random_state=123)\n",
    "\n",
    "gbc_grid3 = GridSearchCV(pipe_gbc3,\n",
    "                       param_grid=param_gb3,\n",
    "                       # GridSearchCV tuning number 3\n",
    "                       refit = 'geometric',\n",
    "                       # Method Validation\n",
    "                       cv=skf)\n",
    "\n",
    "gbc_grid3.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gbc_grid3.best_params_)\n",
    "print(gbc_grid3.best_score_)\n",
    "eval(gbc_grid3.best_estimator_,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning 4 (Learning Rate)\n",
    "# Trying to see what learning rate gives the fastest output. If the learning rate is too small, it takes very long. If too big it also takes too long as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=123)\n",
    "\n",
    "# model yang disiapkan\n",
    "gbc_tune4 = GradientBoostingClassifier(criterion='friedman_mse',\n",
    "#                    learning_rate=0.1,\n",
    "                    max_depth=3,\n",
    "                    max_features=None,\n",
    "                    min_samples_split=2,\n",
    "                    min_samples_leaf=1,\n",
    "                    n_estimators=100,\n",
    "                    subsample=1.0,                  \n",
    "                    random_state=123)\n",
    "\n",
    "select = SelectFromModel(gbc_tune4,threshold='median')\n",
    "pipe_gbc4 = Pipeline([('rus',rus),('select',select),('clf',gbc_tune4)])\n",
    "\n",
    "param_gb4 = {\n",
    "    'clf__learning_rate':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.5,2]\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,random_state=123)\n",
    "\n",
    "gbc_grid4 = GridSearchCV(pipe_gbc4,\n",
    "                       param_grid=param_gb4,\n",
    "                       refit = 'geometric',\n",
    "                       cv=skf)\n",
    "\n",
    "gbc_grid4.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gbc_grid4.best_params_)\n",
    "print(gbc_grid4.best_score_)\n",
    "eval(gbc_grid4.best_estimator_,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Final Model that will be used in this dataset is Gradient Boosting Classifier with Random Under sampling and Stratified 10-Cross Validation with Parameter Tuning. I have made the pipeline for this final model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=123)\n",
    "\n",
    "# model yang disiapkan\n",
    "gbc_tune4 = GradientBoostingClassifier(criterion='friedman_mse',\n",
    "                    learning_rate=0.1,\n",
    "                    max_depth=3,\n",
    "                    max_features=None,\n",
    "                    min_samples_split=2,\n",
    "                    min_samples_leaf=1,\n",
    "                    n_estimators=100,\n",
    "                    subsample=1.0,                  \n",
    "                    random_state=123)\n",
    "\n",
    "select = SelectFromModel(gbc_tune4,threshold='median')\n",
    "pipe_gbc4 = Pipeline([('rus',rus),('clf',gbc_tune4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_gbc4.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = pipe_gbc4.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC-ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def plot_roc_curve(ytest, P_ensemble):\n",
    "    \"\"\"Plot the roc curve for base learners and ensemble.\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, P_ensemble)\n",
    "    plt.plot(fpr, tpr)\n",
    "        \n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(y_test,y_prob[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe_gbc4.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List from Feature that has big affect to model prediction. Feature that affect whether the drivers was dangerous or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from numpy import loadtxt\n",
    "from numpy import sort\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3, stratify=y)\n",
    "rus = RandomUnderSampler()\n",
    "X_sampled,y_sampled = rus.fit_sample(X_train,y_train)\n",
    "gbc = GradientBoostingClassifier()\n",
    "model=gbc.fit(X_sampled,y_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_bar = pd.DataFrame([df_main.columns[indices],importances[indices]]).transpose()\n",
    "to_bar.columns = ['Feature','Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_main.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(20):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, df_main.columns[indices[f]], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "\n",
    "plt.figure(1, figsize=(10, 15))\n",
    "plt.title(\"Feature importances\")\n",
    "ax = sns.barplot(x='Value',y='Feature',data=to_bar[0:30])\n",
    "plt.xlabel('Importances Value', fontsize = 20)\n",
    "plt.ylabel('Feature', fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "# Radian x, then distance is high and it affects the most. This is the graph below.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
